{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0249890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Chem.Descriptors import MolWt\n",
    "from rdkit.Chem.inchi import MolToInchiKey\n",
    "\n",
    "DATA_PATH=\"../data/ReinventResults\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e70877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(file):\n",
    "    df=pd.read_csv(file)\n",
    "    \n",
    "    #create eos_id\n",
    "    num=[f\"{i:05}\" for i in range(len(df))]\n",
    "    prefix=\"EOS-\"\n",
    "    batch=str(batchnum)\n",
    "    eos_id=[prefix + batch +\"-\" + x for x in num]\n",
    "    df[\"EosId\"]=eos_id\n",
    "    \n",
    "    #add inchikey identifier\n",
    "    inchikeys=[]\n",
    "    smiles = [x for x in df['SMILES']]\n",
    "    for smi in smiles:\n",
    "        inchikey=MolToInchiKey(MolFromSmiles(smi))\n",
    "        inchikeys += [inchikey]\n",
    "    df[\"InChIKey\"]=inchikeys \n",
    "    \n",
    "    #add molecular weight\n",
    "    molweights=[]\n",
    "    smiles = [x for x in df['SMILES']]\n",
    "    for smi in smiles:\n",
    "        molweight=MolWt(MolFromSmiles(smi))\n",
    "        molweights += [molweight]\n",
    "    df[\"MolWeight\"]=molweights\n",
    "    \n",
    "    #change total_score style\n",
    "    df=df.rename(columns={\"total_score\":\"TotalScore\", \"SMILES\":\"Smiles\"})\n",
    "    \n",
    "    #add batch of generation \n",
    "    df[\"Batch\"]=batchnum\n",
    "    \n",
    "    #select columns\n",
    "    df=df[[\"EosId\",\"Batch\",\"Smiles\",\"InChIKey\",\"Scaffold\",\"MolWeight\",\"TotalScore\"]]\n",
    "    \n",
    "    #check if there are compounds from original series 4 batch\n",
    "    original=pd.read_csv(\"../data/OriginalData/series4_allsmiles.csv\") #smiles list must be converted to Inchikey\n",
    "    original_inchikeys=[]\n",
    "    smiles = [x for x in original['canonical']]\n",
    "    for smi in smiles:\n",
    "        original_inchikey=MolToInchiKey(MolFromSmiles(smi))\n",
    "        original_inchikeys += [original_inchikey]\n",
    "    \n",
    "    new_inchikeys=df[\"InChIKey\"].tolist()\n",
    "    duplicates=list(set(original_inchikeys).intersection(new_inchikeys))\n",
    "    if not duplicates:\n",
    "        print(\"there are no repeated InChIKeys from original dataset\")\n",
    "        return df\n",
    "    else:\n",
    "        print(str(len(duplicates))+\" InChIKeys will be eliminated from processed results{}\".format(str(batchnum)))\n",
    "        df=df[~df[\"InChIKey\"].isin(original_inchikeys)]\n",
    "        return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a6a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 InChIKeys will be eliminated from processed results0\n",
      "31 InChIKeys will be eliminated from processed results1\n",
      "4 InChIKeys will be eliminated from processed results2\n",
      "there are no repeated InChIKeys from original dataset\n",
      "1 InChIKeys will be eliminated from processed results4\n",
      "2 InChIKeys will be eliminated from processed results5\n"
     ]
    }
   ],
   "source": [
    "for results in os.listdir(DATA_PATH):\n",
    "    if \"results_\" in results:\n",
    "        batchnum=int(results.split(\"results_\")[-1])\n",
    "        results_file=os.path.join(DATA_PATH, results, \"scaffold_memory.csv\")\n",
    "        df=process_results(results_file)\n",
    "        df.to_csv(os.path.join(DATA_PATH, \"ProcessedResults\", \"processed{}.csv\".format(batchnum)),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6aac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches_dict={}\n",
    "for file in os.listdir(os.path.join(DATA_PATH, \"ProcessedResults\")):\n",
    "    if \"processed\" in file:\n",
    "        batchnum=file.split(\"processed\")[-1]\n",
    "        batchnum=int(batchnum.strip(\".csv\"))\n",
    "        df=pd.read_csv(os.path.join(DATA_PATH, \"ProcessedResults\", file))\n",
    "        all_batches_dict[batchnum]=df\n",
    "all_batches_df = pd.concat([df for df in all_batches_dict.values()], ignore_index=True)\n",
    "all_batches_df=all_batches_df.drop_duplicates(subset=\"InChIKey\")\n",
    "all_batches_df.to_csv(os.path.join(DATA_PATH, \"ProcessedResults\", \"all_batches.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7dac5",
   "metadata": {},
   "source": [
    "### Calculate Tanimoto Similarity to series 4 compounds\n",
    "Compare each of the newly generated molecules to all series 4 compounds and keep maximum similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5afc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate maximum similarity of each compound to any series 4 compounds\n",
    "df=pd.read_csv(os.path.join(DATA_PATH,\"ProcessedResults\", \"all_batches.csv\"))\n",
    "s4=pd.read_csv(\"../data/OriginalData/series4_allsmiles.csv\")\n",
    "\n",
    "smiles_list = [x for x in df['Smiles']] #obtain list of smiles for new molecules\n",
    "mols_list=[MolFromSmiles(x) for x in smiles_list] #create list of mols for new molecules\n",
    "fps_list=[FingerprintMols.FingerprintMol(x) for x in mols_list] #create list of fingerprints for new molecules\n",
    "\n",
    "smiles_list_s4 = [x for x in s4['canonical']] #obtain list of smiles for s4\n",
    "mols_list_s4=[MolFromSmiles(x) for x in smiles_list_s4] #create list of mols for s4\n",
    "fps_list_s4=[FingerprintMols.FingerprintMol(x) for x in mols_list_s4] #create list of fingerprints for s4\n",
    "\n",
    "similarity=[] #list to store similarity values\n",
    "for fp in range(len(fps_list)):\n",
    "    individual_sim=[]\n",
    "    for fp_s4 in fps_list_s4:\n",
    "        sim=DataStructs.TanimotoSimilarity(fps_list[fp], fp_s4)\n",
    "        individual_sim += [sim]\n",
    "    max_sim=max(individual_sim)\n",
    "    similarity += [max_sim]\n",
    "\n",
    "\n",
    "df[\"MaxSimToSeries4\"]=similarity\n",
    "\n",
    "df.to_csv(os.path.join(DATA_PATH,\"ProcessedResults\", \"all_batches.csv\"), index=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a825d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41955445544554454"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"MaxSimToSeries4\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba3811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
